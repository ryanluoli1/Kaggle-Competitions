{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP (Multiome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/open-problems-multimodal\"\n",
    "FP_CELL_METADATA = os.path.join(DATA_DIR,\"metadata.csv\")\n",
    "\n",
    "#raw training inputs: gene expressions\n",
    "FP_CITE_TRAIN_INPUTS = os.path.join(DATA_DIR,\"train_cite_inputs.h5\")\n",
    "#raw training targets: protein levels\n",
    "FP_CITE_TRAIN_TARGETS = os.path.join(DATA_DIR,\"train_cite_targets.h5\")\n",
    "#raw test inputs\n",
    "FP_CITE_TEST_INPUTS = os.path.join(DATA_DIR,\"test_cite_inputs.h5\")\n",
    "\n",
    "#raw training inputs: chromatin accessibility\n",
    "FP_MULTIOME_TRAIN_INPUTS = os.path.join(DATA_DIR,\"train_multi_inputs.h5\")\n",
    "#raw training targets: gene expression\n",
    "FP_MULTIOME_TRAIN_TARGETS = os.path.join(DATA_DIR,\"train_multi_targets.h5\")\n",
    "#raw test inputs\n",
    "FP_MULTIOME_TEST_INPUTS = os.path.join(DATA_DIR,\"test_multi_inputs.h5\")\n",
    "\n",
    "#sample submission file\n",
    "FP_SUBMISSION = os.path.join(DATA_DIR,\"sample_submission.csv\")\n",
    "FP_EVALUATION_IDS = os.path.join(DATA_DIR,\"evaluation_ids.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the raw datametadata.csv file \n",
    "metadata_df = pd.read_csv(FP_CELL_METADATA, index_col='cell_id')\n",
    "metadata_df = metadata_df[metadata_df.technology==\"multiome\"]\n",
    "metadata_df.shape\n",
    "\n",
    "#generate donor-time labels for the cells\n",
    "conditions = [\n",
    "    metadata_df['donor'].eq(27678) & metadata_df['day'].eq(2),\n",
    "    metadata_df['donor'].eq(27678) & metadata_df['day'].eq(3),\n",
    "    metadata_df['donor'].eq(27678) & metadata_df['day'].eq(4),\n",
    "    metadata_df['donor'].eq(27678) & metadata_df['day'].eq(7),\n",
    "    metadata_df['donor'].eq(27678) & metadata_df['day'].eq(10),\n",
    "    metadata_df['donor'].eq(13176) & metadata_df['day'].eq(2),\n",
    "    metadata_df['donor'].eq(13176) & metadata_df['day'].eq(3),\n",
    "    metadata_df['donor'].eq(13176) & metadata_df['day'].eq(4),\n",
    "    metadata_df['donor'].eq(13176) & metadata_df['day'].eq(7),\n",
    "    metadata_df['donor'].eq(13176) & metadata_df['day'].eq(10),\n",
    "    metadata_df['donor'].eq(31800) & metadata_df['day'].eq(2),\n",
    "    metadata_df['donor'].eq(31800) & metadata_df['day'].eq(3),\n",
    "    metadata_df['donor'].eq(31800) & metadata_df['day'].eq(4),\n",
    "    metadata_df['donor'].eq(31800) & metadata_df['day'].eq(7),\n",
    "    metadata_df['donor'].eq(31800) & metadata_df['day'].eq(10),\n",
    "    metadata_df['donor'].eq(32606) & metadata_df['day'].eq(2),\n",
    "    metadata_df['donor'].eq(32606) & metadata_df['day'].eq(3),\n",
    "    metadata_df['donor'].eq(32606) & metadata_df['day'].eq(4),\n",
    "    metadata_df['donor'].eq(32606) & metadata_df['day'].eq(7),\n",
    "    metadata_df['donor'].eq(32606) & metadata_df['day'].eq(10),\n",
    "    ]\n",
    "\n",
    "# create a list of the values we want to assign for each condition\n",
    "values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "\n",
    "# create a new column and use np.select to assign values to it using our lists as arguments\n",
    "metadata_df['comb'] = np.select(conditions, values)\n",
    "\n",
    "#reindex the training data\n",
    "X = np.load('../data/multimodal-single-cell-as-sparse-matrix/train_multi_inputs_idxcol.npz', allow_pickle=True)\n",
    "cell_index = X['index']\n",
    "meta = metadata_df.reindex(cell_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('../result/fe/X_164_l2.csv').values\n",
    "Xt = pd.read_csv('../result/fe/Xt_164_l2.csv').values\n",
    "\n",
    "#change the type to float\n",
    "X = X.astype('float32')\n",
    "Xt = Xt.astype('float32')\n",
    "\n",
    "#standardize the target\n",
    "Y = pd.read_hdf(FP_MULTIOME_TRAIN_TARGETS).values\n",
    "Y -= Y.mean(axis=1).reshape(-1, 1)\n",
    "Y /= Y.std(axis=1).reshape(-1, 1)\n",
    "\n",
    "# Read the table of rows and columns required for submission\n",
    "eval_ids = pd.read_parquet(\"../data/multimodal-single-cell-as-sparse-matrix/evaluation.parquet\")\n",
    "\n",
    "\n",
    "# Read the table of rows and columns required for submission\n",
    "eval_ids = pd.read_parquet(\"../data/multimodal-single-cell-as-sparse-matrix/evaluation.parquet\")\n",
    "\n",
    "# Convert the string columns to more efficient categorical types\n",
    "eval_ids.cell_id = eval_ids.cell_id.astype(pd.CategoricalDtype())\n",
    "eval_ids.gene_id = eval_ids.gene_id.astype(pd.CategoricalDtype())\n",
    "\n",
    "# Prepare an empty series which will be filled with predictions\n",
    "submission = pd.Series(name='target',\n",
    "                       index=pd.MultiIndex.from_frame(eval_ids), \n",
    "                       dtype=np.float32)\n",
    "\n",
    "y_columns = np.load(\"../data/multimodal-single-cell-as-sparse-matrix/train_multi_inputs_idxcol.npz\",\n",
    "                   allow_pickle=True)[\"columns\"]\n",
    "\n",
    "test_index = np.load(\".../data/multimodal-single-cell-as-sparse-matrix/test_multi_inputs_idxcol.npz\",\n",
    "                    allow_pickle=True)[\"index\"]\n",
    "\n",
    "cell_dict = dict((k,v) for v,k in enumerate(test_index)) \n",
    "assert len(cell_dict)  == len(test_index)\n",
    "\n",
    "gene_dict = dict((k,v) for v,k in enumerate(y_columns))\n",
    "assert len(gene_dict) == len(y_columns)\n",
    "\n",
    "eval_ids_cell_num = eval_ids.cell_id.apply(lambda x:cell_dict.get(x, -1))\n",
    "eval_ids_gene_num = eval_ids.gene_id.apply(lambda x:gene_dict.get(x, -1))\n",
    "valid_multi_rows = (eval_ids_gene_num !=-1) & (eval_ids_cell_num!=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_score(y_true, y_pred):\n",
    "    if type(y_true) == pd.DataFrame: y_true = y_true.values\n",
    "    if type(y_pred) == pd.DataFrame: y_pred = y_pred.values\n",
    "    corrsum = 0\n",
    "    for i in range(len(y_true)):\n",
    "        corrsum += np.corrcoef(y_true[i], y_pred[i])[1, 0]\n",
    "    return corrsum / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NegativeCorrLoss(preds, targets):\n",
    "    \"Compute the correlation compatible with backpropagation.\"\n",
    "    my = torch.mean(preds, dim=1)\n",
    "    my = torch.tile(torch.unsqueeze(my, dim=1), (1, targets.shape[1]))\n",
    "    ym = preds - my\n",
    "    r_num = torch.sum(torch.multiply(targets, ym), dim=1)\n",
    "    r_den = torch.sqrt(torch.sum(torch.square(ym), dim=1) * float(targets.shape[-1]))\n",
    "    r = torch.mean(r_num / r_den)\n",
    "    return -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, split, X_train, X_val, X_test, y_train, y_val):\n",
    "        self.split = split\n",
    "        if self.split == \"train\":\n",
    "            self.data = X_train\n",
    "            self.gt = y_train\n",
    "        elif self.split == \"val\":\n",
    "            self.data = X_val\n",
    "            self.gt = y_val\n",
    "        else:\n",
    "            self.data = X_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.split == \"train\":\n",
    "            return self.data[idx], self.gt[idx]\n",
    "        elif self.split == \"val\":\n",
    "            return self.data[idx], 0\n",
    "        else:\n",
    "            return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Trainer and Inferer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, dataloaders_dict,  true_test_mod2, scheduler, num_epochs):\n",
    "    \n",
    "    best_mse = 100\n",
    "    best_model = 0\n",
    "    best_cor = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        y_pred = []\n",
    "\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs-1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss, running_corrects = 0.0, 0\n",
    "\n",
    "            for inputs, gts in tqdm(dataloaders_dict[phase]):\n",
    "                \n",
    "                inputs = inputs.cuda()\n",
    "                gts = gts.cuda()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    \n",
    "                    outputs = model(inputs)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss = NegativeCorrLoss(outputs, gts)\n",
    "                        running_loss += loss.item() * inputs.size(0)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    else:\n",
    "                        y_pred.extend(outputs.cpu().numpy())\n",
    "\n",
    "            if phase == \"train\":\n",
    "                epoch_loss = running_loss / len(dataloaders_dict[phase].dataset)\n",
    "                print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "            else:\n",
    "                y_pred = np.array(y_pred)\n",
    "                cor = correlation_score(true_test_mod2, y_pred)\n",
    "                print('cor: ', cor)\n",
    "                if cor > best_cor:\n",
    "                    best_model = copy.deepcopy(model)\n",
    "                    best_cor = cor\n",
    "                    \n",
    "        scheduler.step(cor)\n",
    "        \n",
    "    print(\"Best cor: \", best_cor)\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, dataloader):\n",
    "    \n",
    "    y_pred = []\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for inputs in tqdm(dataloader):\n",
    "        inputs = inputs.cuda()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            y_pred.extend(outputs.cpu().numpy())\n",
    "\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiNet(nn.Module): \n",
    "    \n",
    "    def __init__(self, dim_mod1, dim_mod2):\n",
    "        super(MultiNet, self).__init__()\n",
    "        self.input_ = nn.Linear(dim_mod1, 2048)\n",
    "        self.fc = nn.Linear(2048, 2048)\n",
    "        self.fc1 = nn.Linear(2048, 512)\n",
    "        self.dropout1 = nn.Dropout(p=0.25)\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        self.dropout3 = nn.Dropout(p=0.25)\n",
    "        self.output = nn.Linear(512, dim_mod2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.input_(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.gelu(self.fc(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = F.gelu(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS_ANN = len(meta['comb'].value_counts())\n",
    "kf = GroupKFold(n_splits=N_SPLITS_ANN)\n",
    "\n",
    "y_pred = 0\n",
    "\n",
    "for fold, (idx_tr, idx_va) in enumerate(kf.split(X, groups=meta.comb)):\n",
    "        \n",
    "        X_train = X[idx_tr] \n",
    "        y_train = Y[idx_tr]\n",
    "        X_val = X[idx_va]\n",
    "        y_val = Y[idx_va]   \n",
    "        X_test = Xt\n",
    "        \n",
    "        data = {x: CustomDataset(x, X_train, X_val, X_test, y_train, y_val) for x in ['train', 'val', 'test']}\n",
    "        dataloaders_dict = {\"train\": torch.utils.data.DataLoader(data[\"train\"], \n",
    "                                                                 batch_size=512, \n",
    "                                                                 shuffle=True, \n",
    "                                                                 num_workers=8),\n",
    "                            \"val\": torch.utils.data.DataLoader(data[\"val\"], \n",
    "                                                               batch_size=512, \n",
    "                                                               shuffle=False, \n",
    "                                                               num_workers=8),\n",
    "                            \"test\": torch.utils.data.DataLoader(data[\"test\"], \n",
    "                                                                batch_size=512, \n",
    "                                                                shuffle=False, \n",
    "                                                                num_workers=8)}\n",
    "        \n",
    "        model = None\n",
    "        gc.collect()\n",
    "        \n",
    "        model = MultiNet(164,23418).cuda()\n",
    "\n",
    "        optimizer_ft = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft, mode='max', patience=30, \n",
    "                                                               factor=0.1, verbose=True)\n",
    "        best_model_net = train_model(model, optimizer_ft, dataloaders_dict, y_val, scheduler, num_epochs=100)\n",
    "        \n",
    "        y_pred = y_pred + infer(best_model_net, dataloaders_dict[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.iloc[valid_multi_rows] = y_pred[eval_ids_cell_num[valid_multi_rows].to_numpy(), \n",
    "                                           eval_ids_gene_num[valid_multi_rows].to_numpy()]\n",
    "\n",
    "del eval_ids_cell_num, eval_ids_gene_num, valid_multi_rows, eval_ids, test_index, y_columns\n",
    "gc.collect()\n",
    "\n",
    "submission.reset_index(drop=True, inplace=True)\n",
    "submission.index.name = 'row_id'\n",
    "submission.to_csv('../result/multi/MLP_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
