{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD, PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF trasnformation is a technique for vectorizing text data. TF stands for **term frequency** and IDF stands for **inverse document frequency**. \n",
    "\n",
    "Here, we use the **inversed mean value** over the training set as our IDF:\n",
    "\n",
    "```python\n",
    "IDF = X.shape[0] / X.sum(axis=0)\n",
    "```\n",
    "and the **L1 normalized values** as our TF:\n",
    "\n",
    "```python\n",
    "TF = X / X.sum(axis=1).reshape(-1,1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tfidfTransformer():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.idf = None\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.idf = X.shape[0] / X.sum(axis=0)\n",
    "        self.fitted = True\n",
    "\n",
    "    def transform(self, X):\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError('Transformer was not fitted on any data')\n",
    "        if scipy.sparse.issparse(X):\n",
    "            tf = X.multiply(1 / X.sum(axis=1))\n",
    "            return tf.multiply(self.idf)\n",
    "        else:\n",
    "            tf = X / X.sum(axis=1).reshape(-1,1)\n",
    "            return tf * self.idf\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following codes to perform **`L2 normalization`**:\n",
    "\n",
    "```python\n",
    "normalizer = sklearn.preprocessing.Normalizer(norm=\"l2\")\n",
    "X = normalizer.fit_transform(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use **`log transformation`** to transform skewed data to approximately normal (linear models work better with normally distributed data):\n",
    "\n",
    "```python\n",
    "np.log1p(X)\n",
    "```\n",
    "\n",
    "The above line of code returns the natural logarithm of one plus the input array **log(1 + x)**, element-wise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncated SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following codes to perform **`truncated SVD`** for dimension reduction (down to 512 dimensions):\n",
    "\n",
    "```python\n",
    "pca = sklearn.decomposition.TruncatedSVD(n_components=512, random_state=0)\n",
    "X = pca.fit_transform(X)\n",
    "```\n",
    "\n",
    "SVD is not unique and thus a **random state** was set to ensure reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncated SVD reduces the original data to a smaller subset of features that are **most relevant** to the prediction problem. The reduced dataset is a matrix with a **lower rank** that is said to **approximate** the original matrix.\n",
    "\n",
    "This is done by first performing the standard SVD to decompose a matrix A into:\n",
    "\n",
    "$$A = U \\Sigma V^T$$\n",
    "\n",
    "were $U$ is an **$m×m$** complex unitary matrix, $\\Sigma$ is an **$m×n$** diagonal matrix, and $V$ is an **$m×n$** complex unitary matrix.\n",
    "\n",
    "The diagonal values of $\\Sigma$ are the **singular values** of $A$. The columns of $U$ and $V$ are the **left-singular vectors** and the **right-singular vectors** of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we select the **top k largest** singular values given in $\\Sigma$. These **k columns** can be selected from $\\Sigma$ and **k rows** selected from $V^T$.\n",
    "\n",
    "The **approximate matrix** $B$ is given as:\n",
    "\n",
    "$$A = U \\Sigma_K V_K^T$$\n",
    "\n",
    "In practice, we usually retain and work with a **descriptive subset** (dimensionally reduced) of the data. This is a **dense summary** of the original data matrix:\n",
    "\n",
    "$$T = U \\Sigma_K = AV_K^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose truncated SVD over PCA because it can deal with **sparse matrices**. Therefore, we dont have to **densify** our data matrix which requires a lot of memory. \n",
    "\n",
    "Note: if we **deduct columwise means** from our data, the resultant reduced matrix will be the **same** for truncated SVD and PCA.\n",
    "\n",
    "Why is centering necessary for PCA?\n",
    "\n",
    "- It ensures that the resulting components are only looking at the variance within the dataset, and not capturing the overall mean of the dataset as an important variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of standard SVD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 4, 1, 3, 7],\n",
       "       [9, 4, 2, 6, 5]])"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1,4,1,3,7],[9,4,2,6,5]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, V = np.linalg.svd(X, full_matrices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.51310666, -0.85832485],\n",
       "       [-0.85832485,  0.51310666]])"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.48530309,  5.30810648])"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14.48530309,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  5.30810648,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sigma = np.zeros((X.shape[0], X.shape[1]))\n",
    "Sigma[:X.shape[0], :X.shape[0]] = np.diag(s)\n",
    "Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.56871646, -0.37870979, -0.15393232, -0.46179697, -0.54423237],\n",
       "       [ 0.70828177, -0.26014414,  0.03162869,  0.09488607, -0.64858169],\n",
       "       [-0.12902955, -0.03842183,  0.98656765, -0.04029704, -0.08327995],\n",
       "       [-0.38708866, -0.11526549, -0.04029704,  0.87910888, -0.24983985],\n",
       "       [ 0.09171832, -0.87985314,  0.0191018 ,  0.05730541,  0.46238232]])"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 4., 1., 3., 7.],\n",
       "       [9., 4., 2., 6., 5.]])"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(np.dot(U,Sigma), V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of truncated SVD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 4, 1, 3, 7],\n",
       "       [9, 4, 2, 6, 5]])"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1,4,1,3,7],[9,4,2,6,5]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, V = np.linalg.svd(X, full_matrices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14.48530309,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  5.30810648,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sigma = np.zeros((X.shape[0], X.shape[1]))\n",
    "Sigma[:X.shape[0], :X.shape[0]] = np.diag(s)\n",
    "Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "Sigmak = Sigma[:, :n]\n",
    "Vk = V[:n, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -7.43250547,  -4.55607972],\n",
       "       [-12.43309568,   2.72362478]])"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.dot(SigmaK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -7.43250547,  -4.55607972],\n",
       "       [-12.43309568,   2.72362478]])"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(X, vK.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 4, 1, 3, 7],\n",
       "       [9, 4, 2, 6, 5]])"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1,4,1,3,7],[9,4,2,6,5]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = sklearn.decomposition.TruncatedSVD(n_components=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.43250547,  4.55607972],\n",
       "       [12.43309568, -2.72362478]])"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pca.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 4, 1, 3, 7],\n",
       "       [9, 4, 2, 6, 5]])"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1,4,1,3,7],[9,4,2,6,5]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.41588043e+00,  6.72203811e-17],\n",
       "       [-4.41588043e+00,  6.72203811e-17]])"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pca.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We **`standardize`** the features to avoid the effect of scaling when performing regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
