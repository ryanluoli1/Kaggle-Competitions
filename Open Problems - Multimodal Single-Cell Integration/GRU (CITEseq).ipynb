{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU (CITEseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/open-problems-multimodal\"\n",
    "feature_path = '../data/single_cell_features'\n",
    "\n",
    "train_df = pd.read_feather(feature_path+'train_cite_inputs_id.feather')\n",
    "test_df = pd.read_feather(feature_path+'test_cite_inputs_id.feather')\n",
    "\n",
    "train_cite_X = np.load(feature_path+'train_cite_X.npy')\n",
    "train_cite_y = np.load(feature_path+'train_cite_targets.npy') \n",
    "\n",
    "test_cite_X = np.load(feature_path+'test_cite_X.npy') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_score(y_true, y_pred):\n",
    "    if type(y_true) == pd.DataFrame: y_true = y_true.values\n",
    "    if type(y_pred) == pd.DataFrame: y_pred = y_pred.values\n",
    "    corrsum = 0\n",
    "    for i in range(len(y_true)):\n",
    "        corrsum += np.corrcoef(y_true[i], y_pred[i])[1, 0]\n",
    "    return corrsum / len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to standardize the dataset\n",
    "def zscore(x):\n",
    "    x_zscore = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_row = x[i]\n",
    "        x_row = (x_row - np.mean(x_row)) / np.std(x_row)\n",
    "        x_zscore.append(x_row)\n",
    "    x_std = np.array(x_zscore)    \n",
    "    return x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to compute the cosine similarity between predictions and the truth\n",
    "def cosine_similarity_loss(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    #obtain the row means\n",
    "    mx = tf.reduce_mean(x, axis=1, keepdims=True)\n",
    "    my = tf.reduce_mean(y, axis=1, keepdims=True)\n",
    "    #zero-mean transform the data\n",
    "    xm, ym = x - mx, y - my\n",
    "    #l2 normalization: divid each element by the l2 norm\n",
    "    t1_norm = tf.math.l2_normalize(xm, axis = 1)\n",
    "    t2_norm = tf.math.l2_normalize(ym, axis = 1)a\n",
    "    #compute the cosine similarity\n",
    "    cosine = tf.keras.losses.CosineSimilarity(axis = 1)(t1_norm, t2_norm)\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use to generate batches of data for training the NN\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, train_X, train_y, list_IDs, shuffle, batch_size, labels, ): \n",
    "        self.train_X = train_X\n",
    "        self.train_y = train_y\n",
    "        self.list_IDs = list_IDs        \n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        ct = len(self.list_IDs) // self.batch_size\n",
    "        return ct\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        'Generate one batch of data'\n",
    "        indexes = self.list_IDs[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        #find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        #generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        if self.labels: \n",
    "            return X, y\n",
    "        else: \n",
    "            return X\n",
    " \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle: \n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples'    \n",
    "        X = self.train_X[list_IDs_temp]\n",
    "        y = self.train_y[list_IDs_temp]        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_kfold(train_df, train_cite_X, train_cite_y, test_df, test_cite_X, network, folds, model_name):\n",
    "    \n",
    "    oof_preds = np.zeros((train_df.shape[0],140))\n",
    "    sub_preds = np.zeros((test_df.shape[0],140))\n",
    "    \n",
    "    cv_corr = []\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df,)):          \n",
    "        \n",
    "        print(n_fold)\n",
    "        \n",
    "        train_x = train_cite_X[train_idx]\n",
    "        valid_x = train_cite_X[valid_idx]\n",
    "        train_y = train_cite_y[train_idx]\n",
    "        valid_y = train_cite_y[valid_idx]\n",
    "        train_x_index = train_df.iloc[train_idx].reset_index(drop=True).index\n",
    "        valid_x_index = train_df.iloc[valid_idx].reset_index(drop=True).index\n",
    "        \n",
    "        model = network(train_cite_X.shape[1])\n",
    "        filepath = model_name + '_' + str(n_fold) + '.h5'\n",
    "        es = tf.keras.callbacks.EarlyStopping(patience=10, mode='min', verbose=1) \n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(monitor='val_loss', filepath=filepath, \n",
    "                                                        save_best_only=True, save_weights_only=True, mode='min') \n",
    "        reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=LR_FACTOR, patience=6, verbose=1)\n",
    "    \n",
    "        train_dataset = DataGenerator(\n",
    "            train_x,\n",
    "            train_y,\n",
    "            list_IDs=train_x_index, \n",
    "            shuffle=True, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            labels=True,\n",
    "        )\n",
    "        \n",
    "        valid_dataset = DataGenerator(\n",
    "            valid_x,\n",
    "            valid_y,\n",
    "            list_IDs=valid_x_index, \n",
    "            shuffle=False, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            labels=True,\n",
    "        )\n",
    "    \n",
    "        hist = model.fit(train_dataset,\n",
    "                         validation_data=valid_dataset,  \n",
    "                         epochs=EPOCHS, \n",
    "                         callbacks=[checkpoint,es,reduce_lr_loss],\n",
    "                         workers=4,\n",
    "                         verbose=1)  \n",
    "    \n",
    "        model.load_weights(filepath)\n",
    "        \n",
    "        oof_preds[valid_idx] = model.predict(valid_x, batch_size=BATCH_SIZE, verbose=1)\n",
    "        \n",
    "        oof_corr = correlation_score(valid_y,  oof_preds[valid_idx])\n",
    "        cv_corr.append(oof_corr)\n",
    "        print(cv_corr)       \n",
    "        \n",
    "        sub_preds += model.predict(test_cite_X, batch_size=BATCH_SIZE, verbose=1) / folds.n_splits \n",
    "            \n",
    "        del model\n",
    "        gc.collect()\n",
    "        tf.keras.backend.clear_session()   \n",
    "        \n",
    "    cv = correlation_score(train_cite_y,  oof_preds)\n",
    "    print('Overall:', cv)   \n",
    "    \n",
    "    return oof_preds,sub_preds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cite_cos_sim_model(len_num):\n",
    "    \n",
    "    input_num = tf.keras.Input(shape=(len_num))     \n",
    "    \n",
    "    x = input_num\n",
    "    x0 = tf.keras.layers.Reshape((1,x.shape[1]))(x)\n",
    "    x0 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(1800, activation='elu', \n",
    "                                                           kernel_initializer='Identity', return_sequences=False))(x0)\n",
    "    x1 = tf.keras.layers.GaussianDropout(0.2)(x0)         \n",
    "    x2 = tf.keras.layers.Dense(1800, activation ='elu', kernel_initializer='Identity',)(x1) \n",
    "    x3 = tf.keras.layers.GaussianDropout(0.2)(x2) \n",
    "    x4 = tf.keras.layers.Dense(1800, activation ='elu', kernel_initializer='Identity',)(x3) \n",
    "    x5 = tf.keras.layers.GaussianDropout(0.2)(x4)         \n",
    "    x = tf.keras.layers.Concatenate()([x1,x3,x5])\n",
    "    \n",
    "    output = tf.keras.layers.Dense(140, activation='linear')(x) \n",
    "    \n",
    "    model = tf.keras.models.Model(input_num, output)\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None)\n",
    "    model.compile(loss=cosine_similarity_loss, optimizer=adam)\n",
    "    model.summary()\n",
    "    \n",
    "    plot_model(model, to_file='model1.png', show_shapes=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cite_mse_model(len_num):\n",
    "    \n",
    "    input_num = tf.keras.Input(shape=(len_num))     \n",
    "\n",
    "    x = input_num\n",
    "    x = tf.keras.layers.Dense(1500,activation ='swish',)(x)    \n",
    "    x = tf.keras.layers.GaussianDropout(0.1)(x)   \n",
    "    x = tf.keras.layers.Dense(1500,activation ='swish',)(x) \n",
    "    x = tf.keras.layers.GaussianDropout(0.1)(x)   \n",
    "    x = tf.keras.layers.Dense(1500,activation ='swish',)(x) \n",
    "    x = tf.keras.layers.GaussianDropout(0.1)(x)    \n",
    "    x = tf.keras.layers.Reshape((1,x.shape[1]))(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(700, activation='swish',return_sequences=False))(x)\n",
    "    x = tf.keras.layers.GaussianDropout(0.1)(x)  \n",
    "    \n",
    "    output = tf.keras.layers.Dense(140, activation='linear')(x) \n",
    "\n",
    "    model = tf.keras.models.Model(input_num, output)\n",
    "    opt = tfa.optimizers.AdamW(learning_rate=0.0005, weight_decay=0.0001)    \n",
    "    model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=opt)\n",
    "    model.summary()\n",
    "    \n",
    "    plot_model(model, to_file='model2.png', show_shapes=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Models and Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 620\n",
    "EPOCHS = 100\n",
    "LR_FACTOR = 0.05\n",
    "SEED = 666\n",
    "N_FOLD = 5\n",
    "\n",
    "folds = KFold(n_splits= N_FOLD, shuffle=True, random_state=SEED)     \n",
    "\n",
    "oof_preds_cos, sub_preds_cos = nn_kfold(train_df, \n",
    "                                        train_cite_X, train_cite_y, \n",
    "                                        test_df, test_cite_X, \n",
    "                                        cite_cos_sim_model, folds, 'cite_cos_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 600\n",
    "EPOCHS = 100\n",
    "LR_FACTOR = 0.1\n",
    "SEED = 666\n",
    "\n",
    "folds = KFold(n_splits= 5, shuffle=True, random_state=SEED)    \n",
    "\n",
    "#standardize the training data\n",
    "train_cite_y = zscore(train_cite_y)\n",
    "\n",
    "oof_preds_mse, sub_preds_mse = nn_kfold(train_df, \n",
    "                                        train_cite_X, train_cite_y, \n",
    "                                        test_df, test_cite_X, \n",
    "                                        cite_mse_model, folds, 'cite_mse_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize the training predictions\n",
    "oof_preds_cos = zscore(oof_preds_cos)\n",
    "oof_preds_mse = zscore(oof_preds_mse)\n",
    "\n",
    "#blend the training predictions of the two models\n",
    "oof_preds = oof_preds_cos*0.55 + oof_preds_mse*0.45\n",
    "\n",
    "#computet the blended cv corraltion score\n",
    "cv = correlation_score(train_cite_y,  oof_preds)\n",
    "print('Blend:',cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize the test predictions\n",
    "sub_preds_cos = zscore(sub_preds_cos)\n",
    "sub_preds_mse = zscore(sub_preds_mse)\n",
    "\n",
    "#blend the test predictions \n",
    "sub_preds = sub_preds_cos*0.55 + sub_preds_mse*0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df, test_df, train_cite_X, test_cite_X, train_cite_y\n",
    "del oof_preds_cos, oof_preds_mse, sub_preds_cos, sub_preds_mse\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv( DATA_DIR +'/sample_submission.csv')   \n",
    "submission.loc[:48663*140-1,'target'] = sub_preds.reshape(-1)\n",
    "submission.to_csv(f'../result/cite/GRU_submission.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
