{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature Engineering (Strategy 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm \n",
    "import time \n",
    "import joblib\n",
    "import pathlib\n",
    "import os, random \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42\n",
    "    INPUT = \"../input/amex-data-integer-dtypes-parquet-format\"\n",
    "    TRAIN = True \n",
    "    INFER = True\n",
    "    n_folds = 5\n",
    "    target ='target'\n",
    "    DEBUG= True \n",
    "    ADD_CAT = True\n",
    "    ADD_LAG = True \n",
    "    ADD_DIFF =  [1, 2]\n",
    "    ADD_MIDDLE = True\n",
    "    output_dir = \"./\"\n",
    "\n",
    "path = f'{CFG.INPUT}'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_avg = ['S_2_wk','B_1', 'B_2', 'B_3', 'B_4', 'B_5', 'B_6', 'B_8', 'B_9', 'B_10', 'B_11', 'B_12', 'B_13', \n",
    "                'B_14', 'B_15', 'B_16', 'B_17', 'B_18','B_19', 'B_20', 'B_21', 'B_22', 'B_23', 'B_24', 'B_25', \n",
    "                'B_28', 'B_29', 'B_30', 'B_32', 'B_33', 'B_37', 'B_38', 'B_39', 'B_40', 'B_41', 'B_42','D_53', \n",
    "                'D_54', 'D_55', 'D_58', 'D_59', 'D_60', 'D_61', 'D_62', 'D_65', 'D_66', 'D_69', 'D_70', 'D_71', \n",
    "                'D_72', 'D_73', 'D_74', 'D_75', 'D_76', 'D_77', 'D_78', 'D_80', 'D_82', 'D_84', 'D_86', 'D_91', \n",
    "                'D_92', 'D_94', 'D_96', 'D_103', 'D_104', 'D_108', 'D_112', 'D_113', 'D_114', 'D_115', 'D_117', \n",
    "                'D_118', 'D_119', 'D_120', 'D_121', 'D_122', 'D_123','D_124', 'D_125', 'D_126', 'D_128', 'D_129', \n",
    "                'D_131', 'D_132', 'D_133', 'D_134', 'D_135', 'D_136', 'D_140', 'D_141', 'D_142', 'D_144', 'D_145',\n",
    "                'P_2', 'P_3', 'P_4', 'R_1', 'R_2', 'R_3', 'R_7', 'R_8', 'R_9', 'R_10', 'R_11', 'R_14', 'R_15', 'R_16', \n",
    "                'R_17', 'R_20', 'R_21', 'R_22', 'R_24', 'R_26', 'R_27', 'S_3', 'S_5', 'S_6', 'S_7', 'S_9', 'S_11', 'S_12', \n",
    "                'S_13', 'S_15', 'S_16', 'S_18', 'S_22', 'S_23', 'S_25', 'S_26']\n",
    "\n",
    "spend_p=[ 'S_3',  'S_5', 'S_6', 'S_7', 'S_8', 'S_9', 'S_11', 'S_12', 'S_13', 'S_15', 'S_16', \n",
    "         'S_17', 'S_18', 'S_19', 'S_20', 'S_22', 'S_23', 'S_24', 'S_25', 'S_26', 'S_27']\n",
    "\n",
    "balance_p = ['B_1', 'B_2', 'B_3',  'B_5', 'B_6', 'B_7', 'B_8', 'B_9', 'B_10', 'B_11', 'B_12', \n",
    "             'B_13', 'B_14', 'B_15',  'B_17', 'B_18',  'B_21',   'B_23', 'B_24', 'B_25', 'B_26', \n",
    "             'B_27', 'B_28',  'B_36', 'B_37',  'B_40',    ]\n",
    "\n",
    "payment_p = ['P_2', 'P_3', 'P_4']\n",
    "\n",
    "delq = ['D_39','D_41', 'D_42', 'D_45', 'D_46', 'D_48', 'D_50', 'D_51', 'D_53', 'D_55', 'D_56', 'D_58', \n",
    "        'D_59', 'D_60', 'D_62', 'D_70', 'D_71', 'D_74', 'D_75', 'D_78', 'D_83', 'D_102', 'D_112', 'D_113', \n",
    "        'D_115', 'D_118', 'D_119', 'D_121', 'D_122', 'D_128', 'D_132', 'D_140', 'D_141', 'D_144','D_145'] \n",
    "\n",
    "cat_cols = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120','D_126', 'D_63', 'D_64', 'D_66', 'D_68']     \n",
    "\n",
    "cat_cols_avg = [col for col in cat_cols if col in features_avg]\n",
    "g_num_cols = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df):\n",
    "    '''\n",
    "    1. Drop non-impactful columns\n",
    "    2. Add lag features\n",
    "    3. Add time-seriesfeatures: min, max, mean ect.\n",
    "    '''\n",
    "    df,dgs = preprocess(df) \n",
    "    df = df.drop_duplicates('customer_ID',keep='last')\n",
    "    for dg in dgs:\n",
    "        df = df.merge(dg, on='customer_ID', how='left')\n",
    "    del dgs; gc.collect()    \n",
    "             \n",
    "    diff_cols = [col for col in df.columns if col.endswith('_diff')]\n",
    "    df = df.drop(diff_cols,axis=1)\n",
    "    print(f\"All stats merged {len(df.columns)}\")   \n",
    "  \n",
    "    math_col = globals()['g_num_cols']\n",
    "    for col in spend_p+payment_p+balance_p:\n",
    "        for col_2 in ['min','max']: \n",
    "            if f\"{col}_{col_2}\" in df.columns:\n",
    "                df[f'{col}_{col_2}_lag_sub'] = df[f\"{col}_{col_2}\"] - df[col]\n",
    "                df[f'{col}_{col_2}_lag_div'] = df[f\"{col}_{col_2}\"] / df[col] \n",
    "    print(\"Added more lags\")\n",
    "\n",
    "    df[\"P2B9\"] = df[\"P_2\"] / df[\"B_9\"] \n",
    "    math_col = globals()['g_num_cols']\n",
    "    for pcol in math_col:\n",
    "        if pcol+\"_mean\" in df.columns:  \n",
    "            df[f'{pcol}-mean'] = df[pcol] - df[pcol+\"_mean\"]  \n",
    "            df[f'{pcol}-div-mean'] = df[pcol] /df[pcol+\"_mean\"]\n",
    "        if (pcol+\"_min\" in df.columns) and (pcol+\"_max\" in df.columns):  \n",
    "            df[f'{pcol}_min_div_max'] = df[pcol+\"_min\"] / df[pcol+\"_max\"]  \n",
    "            df[f'{pcol}_min-max'] = df[pcol+\"_min\"] - df[pcol+\"_max\"]\n",
    "    print(f\"Addding col-mean {len(df.columns)} cols {math_col}\")     \n",
    "\n",
    "    drop_col = [col for  col in df.columns if  ((\"sum\" in col))]\n",
    "    print(f\"Dropping {drop_col}\")\n",
    "    df=df.drop(drop_col,axis=1)   \n",
    "\n",
    "    print(f\"Addding col-mean + custom features {len(features_avg)} cols {globals()['g_num_cols']}\")    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    '''\n",
    "    1. Drop columns based on: https://www.kaggle.com/code/raddar/redundant-features-amex/notebook\n",
    "    2. Add features: some statistics on numerical features\n",
    "    3. Add P-S-B features\n",
    "    4. Add lag features\n",
    "    5. Add time-series features: diff\n",
    "    6. Add after-pay features\n",
    "    '''\n",
    "    df['row_id'] = np.arange(df.shape[0])\n",
    "    not_used = get_not_used()\n",
    "    df=df.drop([\"D_103\",\"D_139\"],axis=1)\n",
    "    num_cols = [col for col in df.columns if col not in cat_cols+not_used]   \n",
    "\n",
    "    globals()['g_num_cols'] = num_cols\n",
    "    for col in df.columns:\n",
    "        if col not in not_used+cat_cols:\n",
    "            df[col] = df[col].astype('float32').round(decimals=2).astype('float16') \n",
    "    print(f\"Starting fe [{len(df.columns)}]\") \n",
    "    dgs=add_stats_step(df, num_cols) \n",
    "\n",
    "    train_stat = df.groupby(\"customer_ID\")[spend_p+payment_p+delq+balance_p].agg('sum')\n",
    "    train_stat.columns = [x+'_sum' for x in train_stat.columns]\n",
    "    print(train_stat.columns)\n",
    "    train_stat.reset_index(inplace = True)    \n",
    "    dgs.append(train_stat)\n",
    "    del train_stat; gc.collect() \n",
    "    print(f\"Stats Sum calc [{len(df.columns)}]\")       \n",
    " \n",
    "    df[\"P_SUM\"] = df[payment_p].sum(axis=1) \n",
    "    df[\"S_SUM\"] = df[spend_p].sum(axis=1) \n",
    "    df[\"B_SUM\"] = df[balance_p].sum(axis=1)\n",
    "    df[\"P-S\"] = df.P_SUM - df.S_SUM       \n",
    "    df[\"P-B\"] = df.P_SUM - df.B_SUM\n",
    "    df=df.drop([\"S_SUM\",\"P_SUM\",\"B_SUM\"],axis=1)\n",
    "    print(f\"P-S feature added\")      \n",
    "\n",
    "    if CFG.ADD_LAG:\n",
    "        train_num_agg = df.groupby(\"customer_ID\")[num_cols].agg(['first', 'last']) #payment_p+balance_p+spend_p\n",
    "        train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n",
    "        train_num_agg.reset_index(inplace = True) \n",
    "        for col in train_num_agg:\n",
    "            if 'last' in col and col.replace('last', 'first') in train_num_agg:\n",
    "                train_num_agg[col + '_lag_sub'] = train_num_agg[col] - train_num_agg[col.replace('last', 'first')]\n",
    "                train_num_agg[col + '_lag_div'] = train_num_agg[col] / train_num_agg[col.replace('last', 'first')]            \n",
    "        train_num_agg.drop([col for col in train_num_agg.columns if \"last\" in col],axis=1, inplace=True)\n",
    "        dgs.append(train_num_agg)\n",
    "        del train_num_agg\n",
    "        print(f\"Computing diff 1 features ,curr cols [{len(df.columns)}]\") \n",
    "        dff_cols =  payment_p+balance_p+spend_p+delq ## Replace with num_cols\n",
    "      \n",
    "      for pdf in CFG.ADD_DIFF:\n",
    "        train_diff = get_difference(df, dff_cols,period=pdf)\n",
    "        print(f\"Computing Diff {pdf} ,curr cols [{ train_diff.columns}]\") \n",
    "        dgs.append(train_diff)    \n",
    "        del train_diff; gc.collect()             \n",
    "    \n",
    "    for bcol in [f'B_{i}' for i in [11,14,17]]+['D_39','D_131']+[f'S_{i}' for i in [16,23]]:\n",
    "        for pcol in ['P_2','P_3']:\n",
    "            if bcol in df.columns:\n",
    "                df[f'{bcol}-{pcol}'] = df[bcol] - df[pcol]\n",
    "\n",
    "    df['S_2'] = pd.to_datetime(df['S_2'])\n",
    "    df['cid'], _ = df.customer_ID.factorize()    \n",
    "\n",
    "    s2_count = df[df.S_2.dt.dayofweek == 6].groupby(\"customer_ID\")['S_2'].agg(['count']) \n",
    "    s2_count.columns = ['S_2_Sun_Count']\n",
    "    s2_count.reset_index(inplace = True)     \n",
    "    dgs.append(s2_count)\n",
    "    print(f\"sundays count added and calculated [{len(s2_count.columns)}]\") \n",
    "\n",
    "    df['S_2_wk'] =  df['S_2'].dt.week\n",
    "    s2_count = df.groupby(\"customer_ID\")['S_2_wk'].agg(['std'])  \n",
    "    s2_count.columns = ['S_2_wk_std']\n",
    "    s2_count.reset_index(inplace = True)     \n",
    "    dgs.append(s2_count)\n",
    "    df=df.drop([\"S_2_wk\"],axis=1 )\n",
    "    print(f\"sundays count added and calculated [{len(s2_count.columns)}]\")        \n",
    "    del s2_count; gc.collect()     \n",
    "\n",
    "\n",
    "    if CFG.ADD_CAT: \n",
    "        train_cat_agg = df.groupby(\"customer_ID\")[cat_cols].agg(['count', 'nunique', 'std','first']) \n",
    "        train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n",
    "        train_cat_agg.reset_index(inplace = True)     \n",
    "        dgs.append(train_cat_agg)\n",
    "        del train_cat_agg; gc.collect() \n",
    "        train_cat_mean = df.groupby(\"customer_ID\")[cat_cols_avg].agg(['mean']) \n",
    "        train_cat_mean.columns = ['_'.join(x) for x in train_cat_mean.columns]\n",
    "        train_cat_mean.reset_index(inplace = True)    \n",
    "        print(f\"Added cat mean cols [{train_cat_mean.columns}]\")   \n",
    "        dgs.append(train_cat_mean)\n",
    "        del train_cat_mean; gc.collect() \n",
    "        print(f\"CAT features added {len(df.columns)}\") \n",
    "\n",
    "    s2_count = df.groupby(\"customer_ID\")['S_2'].agg(['count']) \n",
    "    s2_count.columns = ['S_2_Count']\n",
    "    s2_count.reset_index(inplace = True)    \n",
    "    df = df.merge(s2_count, on='customer_ID', how='inner')\n",
    "    print(f\"Stats added and calculated [{len(s2_count.columns)}]\")    \n",
    "    del s2_count; gc.collect() \n",
    "\n",
    "    if CFG.ADD_MIDDLE:\n",
    "        f_middle = df[df.S_2_Count > 2].groupby(['customer_ID'])[balance_p+payment_p+delq+spend_p].apply(lambda x: x.iloc[(len(x)+1)//2])   \n",
    "        df_middle.columns = [x+'_mid' for x in df_middle.columns]  \n",
    "        dgs.append(df_middle) \n",
    "        print(f\"Mid Cols added [{len(df_middle.columns)}]\")    \n",
    "        del df_middle; gc.collect() \n",
    "      \n",
    "    df = df.sort_values('row_id')\n",
    "    df = df.drop(['row_id'],axis=1)\n",
    "\n",
    "    return df, dgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_not_used():  \n",
    "    return ['row_id', 'customer_ID', 'target', 'cid', 'S_2','D_103','D_139']    \n",
    "\n",
    "def add_stats_step(df, cols):\n",
    "    n = 50\n",
    "    dgs = []\n",
    "    for i in range(0,len(cols),n):\n",
    "        s = i\n",
    "        e = min(s+n, len(cols))\n",
    "        dg = add_stats_one_shot(df, cols[s:e])\n",
    "        dgs.append(dg)\n",
    "    return dgs\n",
    "\n",
    "stats = ['mean', 'min', 'max','std'] \n",
    "def add_stats_one_shot(df, cols):\n",
    "    dg = df.groupby('customer_ID').agg({col:stats for col in cols})\n",
    "    out_cols = []\n",
    "    for col in cols:\n",
    "        out_cols.extend([f'{col}_{s}' for s in stats])\n",
    "    dg.columns = out_cols\n",
    "    dg = dg.reset_index()\n",
    "    return dg\n",
    "\n",
    "def get_difference(data, num_features,period=1): \n",
    "    df1 = []\n",
    "    customer_ids = []\n",
    "    for customer_id, df in  data.groupby(['customer_ID']):\n",
    "        diff_df1 = df[num_features].diff(period).iloc[[-1]].values.astype(np.float32)\n",
    "        df1.append(diff_df1)\n",
    "        customer_ids.append(customer_id)\n",
    "    df1 = np.concatenate(df1, axis = 0)\n",
    "    df1 = pd.DataFrame(df1, columns = [col + f'_diff{period}' for col in df[num_features].columns])\n",
    "    df1['customer_ID'] = customer_ids\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(f'{CFG.INPUT}/train.parquet') \n",
    "train = process_data(train) \n",
    "trainl = pd.read_csv(f'../input/amex-default-prediction/train_labels.csv')\n",
    "trainl.target = trainl.target.astype('int8')  \n",
    "train = train.merge(trainl, on='customer_ID', how='left')\n",
    "train.to_pickle(f\"train_fe_v1.pickle\")\n",
    "print(\"Saving train FE to file\") \n",
    "\n",
    "\n",
    "test = pd.read_parquet(f'{CFG.INPUT}/test.parquet') \n",
    "test = process_data(test) \n",
    "test.to_pickle(f\"test_fe_v1.pickle\")\n",
    "print(\"Saving test FE to file\")   \n",
    "print('FE finished')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
